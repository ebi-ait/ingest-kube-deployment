# Ingest Backup

## Contents

* [Introduction](#introduction)
* [Assumptions](#assumptions)
* [Usage](#usage)
* [Development](#development)

## <a name="introduction"></a>Introduction
The backup strategies devised in here are to ensure availability, and prevent significant loss of data after they have been ingested through DCP infrastructure. The utility is meant to complement a more robust way to persist data on Ingest's deployment environment.

While the strategies were created with HCA DCP specifically in mind, the tools may be reused to accommodate any system that uses Mongo DB deployed as part of of a cluster of Docker containers (with some modifications).

## <a name="assumptions"></a>Assumptions
Ingest infrastructure is deployed as a system of multiple self contained microservices through Docker with the use of Kubernetes. The backup system is deployed as a Docker container that has direct access to a predefined Kubernetes service, named `mongo-service` from which it gets the data it backs up to an S3 bucket defined through the `S3_BUCKET` environment variable.


## <a name="usage"></a>Usage

### Quickstart

Ingest Backup is deployed through Helm on Kubernetes through the `setup.sh` script provided in this module. The setup script [requires AWS access keys and role ARN](#credentials) can be downloaded through the AWS console in CSV format. With the addition of the [automated verification procedure](#verification), the setup script also requires the Slack Webhook URL where alerts can be sent. This can be retrieved through Slack apps management on their Web site.

Before running the script, the environment variable `DEPLOYMENT_STAGE` must be set as the setup operation uses it to determine which configuration to use. This variable is, at the time of writing, set when switching environments through Ingest deployment procedures.

To deploy Ingest Backup, the script is run with the CSV file, the AWS role ARN, and the Slack Webhook URL:

```
./setup.sh /path/to/aws_access_credentials.csv role_ARN slack_webhook_url
```



### Backing Up
By default, the backup system is set to run every day at 12am. This can be configured by updating the `cronSchedule` config in the respective deployment environment value file in the `config` directory to match another preferred schedule. Alternatively, for instances of ingest backup already deployed through Kubernetes, the schedule can be patched through the `reschedule.sh` script provided in this module. The script takes a cron format schedule string:

```
./reschedule '*/5 * * * * '
```

Running the sample above will cause the backup procedure to run every five minutes (which is probably a little too frequent). The script works by, essentially, updating the `spec.schedule` property of the cron job:

```
kubectl patch cronjob ingest-backup -p '{  "spec": { "schedule": "*/5 * * * *"  }  }'
```

#### <a name="credentials"></a>Security Credentials
The backup system uses Amazon's AWS CLI tools to copy data to AWS. As the backup data will be dumped into a remote S3 bucket, the process running the backups needs to be configured to have access to the bucket in question. Security credentials should be set through the environment variables to get the backup system working correctly. AWS provides documentation on [how to setup security credentials for the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html).

The backup process runs by assuming an AWS role to be able to access the the S3 bucket. The role ARN is also required on start up.

#### Environment Variables
Several environment variables are defined as part of the configuration of the running container.

* `AWS_ACCESS_KEY_ID` - the access key ID generated by AWS for the IAM user running the backup utility
* `AWS_SECRET_ACCESS_KEY` - the secret access key generated by AWS for the IAM user running the backup utility
* `S3_BUCKET` - the name of the AWS S3 to which the backup data will be moved
* `BACKUP_DIR` - a subdirectory in the S3 bucket to which the data will be copied. This is useful for when there are multiple environments sharing the same S3 bucket for backup (e.g. dev, integration, production)

The first 2 environment variables above (access keys) are directly exposed variables used by AWS client itself.

#### Backup Data
The backup system takes the output of Mongo's `mongodump` utility and puts them into a compressed directory (tarball), which are moved to the specified S3 bucket. They are preserved in format that Mongo utilities should be able to process and understand.

### Restoring Data
The compressed directories of database backups available in the S3 bucket can be decompressed using the `tar` utility as follows:

    tar -xzvf 2018-04-04T11_37.tar.gz

This will create a directory structure, `data/db/dump/2018-04-04T11_37`, which contains the output of the `mongodump`. The `tar` utility provides more options documented in its manual (`man tar`).

To restore the backup data, the `mongorestore` utility is used:

    mongorestore 2018-04-04T11_37

The [official documentation for `mongorestore`](https://docs.mongodb.com/manual/reference/program/mongorestore/) tool lists more options for customising the restoration process.

### Verifying Restoration

(Note: a step by step guide of the verification process for Ingest has been documented [here](https://docs.google.com/document/d/1y2pgzoK2Xt7ZCGVt_big7Lfto5nSKvNNSc70yU38t0Y/edit?usp=sharing).)

To check if the backups contain correct information, the following general strategy may be adopted:

1) Connect to source Mongo DB instance, through a shell for example. As the Mongo instance is on a Kubernetes cluster, the shell can be invoked through the `exec` utility of `kubectl`:

        kubectl -n <namespace> exec -it <mongo-pod> -- /bin/bash

2) Create a new Mongo DB instance to which the backup data will be restored. This process can easily be done by running a new Mongo container through Docker. It is advised that when running a new Mongo instance for testing the backup data is first decompressed (using tools like `tar` described above), and the resulting directory is used as a host volume:

        docker run -d --name mongo-test -v $PWD/data/db:/data/db mongo

3) Connect to the new Mongo instance through the Docker exec utility:

        docker exec -it mongo-test /bin/bash

4) While connected to the new container hosting the new Mongo DB instance, the backup can be restored through `mongorestore` tool:

        mongorestore /data/db/dump/2018-04-04T11_37

5) To verify that the restored data is consistent with the source, connect to the new Mongo DB instance (perhaps using `mongo` client) and verify that the data match with the source. As a simple test, `show collections` should display the same collections in both the source DB and the new one. Each collection in the source should contain the same number of documents as its counterpart on the new DB. This can be checked using the `count` method of each collection:

        db.<collection_name>.count()

#### <a name="verification"></a>Verification Procedure

Backups are automatically verified through the verification script that runs as part of the Helm installation. By default this script runs 30 minutes after the backup script is initiated. Depending on how large the batch of data being backed up is, this may need to be adjusted.


### Backup expiration policy in S3
A lifecycle rule has been set up in S3 that expires (permanently deletes) backups after 90 days. New backups are by default tagged with `"auto-delete": "true"`. The expiration policy only applies to files with this tag which - if needed - may be changed to `"auto-delete": "false"` by a user who has access to the S3 console to allow a backup to persist (not expire).


## <a name="development"></a>Development

### Directory structure

```
backup
├── Dockerfile-backup
├── Dockerfile-base
├── Dockerfile-verify
├── backup
│   ├── ...
│   ├── templates
│   └── values.yaml
├── compose-backup.yml
├── compose-verify.yml
├── config
│   ├── dev.yaml
│   ├── integration.yaml
│   ├── prod.yaml
│   └── staging.yaml
├── export_aws_credentials.sh
├── reschedule.sh
├── setup.sh
└── src
    ├── aws_setup
    ├── backup.sh
    ├── restore.sh
    └── verify_backup
```

#### Docker Files

The backup operation comprises of 2 main components, `backup`, which, as name suggests, does the actual backing up of data, and `backup-verify`, which does the checking of the backups. Each of these is built as Docker image based on another custom component, the `backup-base`.

#### Helm Chart

Backup operations (both backup and verify) are packaged as a "cloud application" through Helm. The application package includes backup, verify, and secrets, which contains credentials for accessing AWS. The Helm chart and all related manifests are contained in the `backup` directory.

##### Configuration

Deploying the backup system through Helm requires some configuration. The Helm chart contains default values in the `values.yml` that attempts to exhaustively list all the configurable parameters. The default values are provided based on reasonable assumptions across all deployment environments. However, as there are limitations on what can be tracked through the version control system, some values are expected to be provided on deployment time, most of which are security related. Most of the parameters that are expected to be filled on deployment time are given `default` values.

Deployment environment specific configuration can be provided through respective configuration file in the `config` directory found in the backup root directory. The names of the configuration files match exactly all the possible values of `DEPLOYMENT_STAGE` variable used in more general Ingest deployment through the root `ingest-kube-deployment` directory. `DEPLOYMENT_STAGE` is used by the setup script `setup.sh` to determine which custom configuration to use.

#### Backup Scripts

The core scripts for running all logic involved in backing up Ingest data are located in the `src` directory. At the top level are the entry points for backup, and restoration components, `backup.sh` and `restore.sh` respectively. The restoration operation involves verification of data restored in an internal replica database. Verification is handled through scripts in `verify_backup` directory.

### Testing Backup Scripts

In order to facilitate testing for the scripts included in this module, utility scripts and deployment manifests have been set up. For example, to locally run the verification operation, `docker-compose` can be used with the `compose-verify.yml` manifest. Some environment variables can only be specified on run time as they cannot be checked into the version control system. For this, scripts like `export_aws_credentials.sh` can be utilised. Below is an example local test run of the verification operation:

    source ./export_aws_credentials.sh ~/path/to/aws_keys.csv; &&\
    AWS_ROLE_ARN=arn:aws:iam::1234:role/developer \
    SLACK_WEBHOOK=http://slack/webhook/url \
    docker-compose -f compose-verify.yml up --build &&\
    docker-compose -f compose-verify.yml rm -fsv
